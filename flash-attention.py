"""
TODO:
Simplified toy implmentation
non-causal attention
Fixed block sizes (BLOCK_M=64, BLOCK_N=64)
Simple pointer-based memory access
Test correctness vs PyTorch's attention


TODO:
Add causal masking
Experiment with block sizes
Add timing benchmarks

TODO:
Try different BLOCK_M/BLOCK_N combinations
Profile memory access patterns
Compare performance
"""